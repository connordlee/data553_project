{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----- Importing all required Packages -----##\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----- Loading all training data -----##\n",
    "cwd = os.getcwd()\n",
    "train_bug = pd.read_json(cwd+'/training_data/Bug_tt.json')\n",
    "train_feature = pd.read_json(cwd+'/training_data/Feature_tt.json')\n",
    "train_rating = pd.read_json(cwd+'/training_data/Rating_tt.json')\n",
    "train_userex = pd.read_json(cwd+'/training_data/UserExperience_tt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----- Creating flags as required & setting NaN values to 0 -----##\n",
    "train_bug['Bug'] = np.where(train_bug['label']==\"Bug\", 1, 0)\n",
    "train_feature['Feature'] = np.where(train_feature['label']==\"Feature\", 1, 0)\n",
    "train_rating['Rating'] = np.where(train_rating['label']==\"Rating\", 1, 0)\n",
    "train_userex['Userex'] = np.where(train_userex['label']==\"UserExperience\",1,0)\n",
    "\n",
    "train_bug = train_bug.fillna(0)\n",
    "train_feature = train_feature.fillna(0)\n",
    "train_rating = train_rating.fillna(0)\n",
    "train_userex = train_userex.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----- Creating function for Lematizing Comments/Text -----##\n",
    "def lemmatizeData(commentSeries):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [token for token in commentSeries.split(\" \") if token != \"\"]\n",
    "    output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----- Reading in Testing data -----##\n",
    "test_data = pd.read_excel('D:\\\\UBC Assignments\\\\data553_project\\\\Connor_Christian_ClassificationChecked.xlsx')\n",
    "test_data=test_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----- Lemmatizing training and test data\n",
    "lemmatized_bug = train_bug.copy()\n",
    "lemmatized_feature = train_feature.copy()\n",
    "lemmatized_userex = train_userex.copy()\n",
    "lemmatized_rating = train_rating.copy()\n",
    "lemmatized_test = test_data.copy()\n",
    "\n",
    "lemmatized_feature[\"comment\"] = lemmatized_feature[\"comment\"].apply(lemmatizeData)\n",
    "lemmatized_bug[\"comment\"] = lemmatized_bug[\"comment\"].apply(lemmatizeData)\n",
    "lemmatized_userex[\"comment\"] = lemmatized_userex[\"comment\"].apply(lemmatizeData)\n",
    "lemmatized_rating[\"comment\"] = lemmatized_rating[\"comment\"].apply(lemmatizeData)\n",
    "lemmatized_test[\"text\"] = lemmatized_test[\"text\"].apply(lemmatizeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_lemmatize_ratings(train_df,type,test_df):\n",
    "    count_vect = CountVectorizer()\n",
    "    bow_train = sp.sparse.hstack((count_vect.fit_transform(train_df.comment),train_df[['rating']].values),format='csr')\n",
    "    bow_test = sp.sparse.hstack((count_vect.transform(test_df.text),test_df[['score_x']].values),format='csr')\n",
    "    if type == 'Feature':\n",
    "        model_bow = GaussianNB().fit(bow_train.todense(),train_df.Feature)\n",
    "        bow_pred = model_bow.predict(bow_test.todense())\n",
    "    elif type == 'Bug':\n",
    "        model_bow = GaussianNB().fit(bow_train.todense(),train_df.Bug)\n",
    "        bow_pred = model_bow.predict(bow_test.todense())\n",
    "    elif type == 'Rating':\n",
    "        model_bow = GaussianNB().fit(bow_train.todense(),train_df.Rating)\n",
    "        bow_pred = model_bow.predict(bow_test.todense())\n",
    "    elif type == 'Userex':\n",
    "        model_bow = GaussianNB().fit(bow_train.todense(),train_df.Userex)\n",
    "        bow_pred = model_bow.predict(bow_test.todense())\n",
    "    f1 = f1_score(test_df[type],bow_pred)\n",
    "    recall = recall_score(test_df[type],bow_pred)\n",
    "    precision = precision_score(test_df[type],bow_pred)\n",
    "    metrics = [type,precision,recall,f1]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blr_feature_score = get_bow_lemmatize_ratings(lemmatized_feature, 'Feature', lemmatized_test)\n",
    "blr_bug_score = get_bow_lemmatize_ratings(lemmatized_bug,'Bug',lemmatized_test)\n",
    "blr_userex_score = get_bow_lemmatize_ratings(lemmatized_userex,'Userex',lemmatized_test)\n",
    "blr_rating_score = get_bow_lemmatize_ratings(lemmatized_rating, 'Rating',lemmatized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Feature', 0.11627906976744186, 0.5555555555555556, 0.19230769230769232]\n",
      "['Bug', 0.1956521739130435, 0.6428571428571429, 0.30000000000000004]\n",
      "['Userex', 0.2926829268292683, 0.3870967741935484, 0.3333333333333333]\n",
      "['Rating', 0.7682926829268293, 0.7875, 0.7777777777777778]\n"
     ]
    }
   ],
   "source": [
    "print(blr_feature_score)\n",
    "print(blr_bug_score)\n",
    "print(blr_userex_score)\n",
    "print(blr_rating_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
